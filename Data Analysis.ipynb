{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51000c",
   "metadata": {},
   "source": [
    "## Normalized City Home Values vs Fed Balance Sheet Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of DataFrame with specific columns using iloc\n",
    "columns_to_select = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "subset_df = City_zhvi_by_date.iloc[:, columns_to_select]\n",
    "# rename columns\n",
    "colum = ['Date', 'New York', 'Los Angeles', 'Houston', 'Chicago', 'San Antonio', 'Philadelphia', 'Phoenix', 'Las Vegas', 'San Diego', 'Dallas']\n",
    "subset_df.columns = colum\n",
    "subset_df.set_index('Date', inplace=True)\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "fed_balance = Path('../../total_assets.csv')\n",
    "fed_df = pd.read_csv(fed_balance)\n",
    "fed_df['date'] = pd.to_datetime(fed_df['date'])\n",
    "fed_c = ['Date', 'Total_Assets']\n",
    "fed_df.columns = fed_c\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "fed_df['Date'] = fed_df['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "fed_df.set_index('Date', inplace=True)\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate\n",
    "Homes_v_Fed = pd.concat([fed_df, subset_df], axis='columns', join='inner')\n",
    "Homes_v_Fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization formula\n",
    "homes_v_fed_norm = (Homes_v_Fed - Homes_v_Fed.min()) / (Homes_v_Fed.max() - Homes_v_Fed.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot for Normalized Asset Price of each column\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Total_Assets'], label='Fed_Total_Assets')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['New York'], label='New York')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Los Angeles'], label='Los Angeles')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Houston'], label='Houston')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Chicago'], label='Chicago')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['San Antonio'], label='San Antonio')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Philadelphia'], label='Philadelphia')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Phoenix'], label='Phoenix')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Las Vegas'], label='Las Vegas')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['San Diego'], label='San Diego')\n",
    "plt.plot(homes_v_fed_norm.index, homes_v_fed_norm['Dallas'], label='Dallas')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Normalized Monthly Home Values Compared to Fed Balance Sheet')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('Normalized City Home Values vs Fed Balance Sheet Monthly.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274e6e5",
   "metadata": {},
   "source": [
    "## Scatter plot of New York home values vs Normalized Fed Balance Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Homes_v_Fed.plot(kind='scatter', x='Total_Assets', y='New York')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1310a1",
   "metadata": {},
   "source": [
    "## Correlation of Raw Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eca575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of raw prices\n",
    "price_correlation = Homes_v_Fed.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "# save plot\n",
    "plt.savefig('seaborn_plot.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dbf8a",
   "metadata": {},
   "source": [
    "## Monthly Cumulative Gain Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvf = Homes_v_Fed\n",
    "# Calculate the monthly percent gains\n",
    "Fed_Monthly_Return = Homes_v_Fed['Total_Assets'].pct_change()\n",
    "NY_Monthly_Return = Homes_v_Fed['New York'].pct_change()\n",
    "LA_Monthly_Return = Homes_v_Fed['Los Angeles'].pct_change()\n",
    "Htown_Monthly_Return = Homes_v_Fed['Houston'].pct_change()\n",
    "Chi_Monthly_Return = Homes_v_Fed['Chicago'].pct_change()\n",
    "SA_Monthly_Return = Homes_v_Fed['San Antonio'].pct_change()\n",
    "Phi_Monthly_Return = Homes_v_Fed['Philadelphia'].pct_change()\n",
    "Pho_Monthly_Return = Homes_v_Fed['Phoenix'].pct_change()\n",
    "LV_Monthly_Return = Homes_v_Fed['Las Vegas'].pct_change()\n",
    "SD_Monthly_Return = Homes_v_Fed['San Diego'].pct_change()\n",
    "Dallas_Monthly_Return = Homes_v_Fed['Dallas'].pct_change()\n",
    "# Calculate the cumulative gain (or loss) and append to the hvf dataframe, using the above values\n",
    "hvf['Fed_Cumulative_Gain'] = (1 + Fed_Monthly_Return).cumprod() - 1\n",
    "hvf['NYC_Cumulative_Gain'] = (1 + NY_Monthly_Return).cumprod() - 1\n",
    "hvf['LA_Cumulative_Gain'] = (1 + LA_Monthly_Return).cumprod() - 1\n",
    "hvf['Houston_Cumulative_Gain'] = (1 + Htown_Monthly_Return).cumprod() - 1\n",
    "hvf['Chicago_Cumulative_Gain'] = (1 + Chi_Monthly_Return).cumprod() - 1\n",
    "hvf['San_Antonio_Cumulative_Gain'] = (1 + SA_Monthly_Return).cumprod() - 1\n",
    "hvf['Philadelphia_Cumulative_Gain'] = (1 + Phi_Monthly_Return).cumprod() - 1\n",
    "hvf['Phoenix_Cumulative_Gain'] = (1 + Pho_Monthly_Return).cumprod() - 1\n",
    "hvf['Las_Vegas_Cumulative_Gain'] = (1 + LV_Monthly_Return).cumprod() - 1\n",
    "hvf['San_Diego_Cumulative_Gain'] = (1 + SD_Monthly_Return).cumprod() - 1\n",
    "hvf['Dallas_Cumulative_Gain'] = (1 + Dallas_Monthly_Return).cumprod() - 1\n",
    "# drop unnecessary columns (cleared because I already ran it)\n",
    "# hvf = hvf.drop(columns=['Total_Assets', 'New York', 'Los Angeles', 'Houston', 'Chicago', 'San Antonio', 'Philadelphia', 'Phoenix', 'Las Vegas', 'San Diego', 'Dallas'])\n",
    "hvf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization formula for cumulative gains dataframe (hvf)\n",
    "hvf_norm = (hvf - hvf.min()) / (hvf.max() - hvf.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aade12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plots for Normalized monthly cumulative gains\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(hvf_norm.index, hvf_norm['Fed_Cumulative_Gain'], label='Fed_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['NYC_Cumulative_Gain'], label='NYC_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['LA_Cumulative_Gain'], label='LA_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['Houston_Cumulative_Gain'], label='Houston_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['Chicago_Cumulative_Gain'], label='Chicago_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['San_Antonio_Cumulative_Gain'], label='San_Antonio_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['Philadelphia_Cumulative_Gain'], label='Philadelphia_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['Phoenix_Cumulative_Gain'], label='Phoenix_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['Las_Vegas_Cumulative_Gain'], label='Las_Vegas_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['San_Diego_Cumulative_Gain'], label='San_Diego_Cumulative_Gain')\n",
    "plt.plot(hvf_norm.index, hvf_norm['Dallas_Cumulative_Gain'], label='Dallas_Cumulative_Gain')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Gain')\n",
    "plt.title('Monthly Cumulative Gain Comparison')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00da906",
   "metadata": {},
   "source": [
    "## Correlation heatmap for cumulative gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_correlation = hvf.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff356a",
   "metadata": {},
   "source": [
    "## Liquidity vs Assets Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee2b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidpath = Path(\"corporate_liquidity.csv\")\n",
    "liquid_df = pd.read_csv(liquidpath, parse_dates=True, index_col=\"date\", infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6829fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "walclpath = Path('total_assets.csv')\n",
    "assets_df = pd.read_csv(walclpath, parse_dates=True, index_col=\"date\", infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d85c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidity_assets = pd.concat([liquid_df, assets_df], axis=1, join='inner')\n",
    "liquidity_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = liquidity_assets.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca847ed",
   "metadata": {},
   "source": [
    "## Effective Rate vs Liquidity Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_path = Path('effective_rate.csv')\n",
    "effective_rate = pd.read_csv(rate_path, parse_dates=True, index_col=\"date\", infer_datetime_format=True)\n",
    "effective_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d68e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_liquidity = pd.concat([liquid_df, effective_rate], axis=1, join='inner')\n",
    "effective_liquidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f68536",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = effective_liquidity.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbed21",
   "metadata": {},
   "source": [
    "## 30 year bond vs Home Prices Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aba2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "condopath = Path('city_condo_smoothed_by_date.csv')\n",
    "condos = pd.read_csv(condopath)\n",
    "columns_to_select = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "condos = condos.iloc[:, columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fb3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_yield_30.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condos_by_bond = pd.concat([bond_yield_30, condos], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf584eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "condos_by_bond = condos_by_bond.drop(['date', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4be9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "top_condos_by_bond = condos_by_bond.iloc[:, columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d517836",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = top_condos_by_bond.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig(\"30_bond_vs_homeprice_cities.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fc7ea",
   "metadata": {},
   "source": [
    "## SP500 vs Condo prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = sp500.drop(['High', 'Low', 'Close', 'Adj Close', 'Volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "condos_by_sp500 = pd.concat([condos, sp500], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac97138",
   "metadata": {},
   "outputs": [],
   "source": [
    "condos_by_sp500 = condos_by_sp500.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = condos_by_sp500.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig('sp500_vs_homeprice_cities.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8b132",
   "metadata": {},
   "source": [
    "## Price of Gold vs Condo prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9157e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = data.drop(['High', 'Low', 'Close', 'Adj Close', 'Volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438639d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_by_condos = pd.concat([gold, condos], axis=1, join='inner')\n",
    "gold_by_condos = gold_by_condos.drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = gold_by_condos.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig('gold_by_condos.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec01eb",
   "metadata": {},
   "source": [
    "## Fed Liquid Assets vs Condo Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('corporate_liquidity (1).csv')\n",
    "corp_liquid = pd.read_csv(filepath)\n",
    "corp_liquid.rename(columns={'date': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_liquid_condos = pd.concat([condos, corp_liquid], axis=1, join='inner')\n",
    "corp_liquid_condos.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ba7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = corp_liquid_condos.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig('lqd_assets_v_homeprices.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceefaf2",
   "metadata": {},
   "source": [
    "## Effective Rate vs Condo prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9af526",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('effective_rate (1).csv')\n",
    "eff_rate = pd.read_csv(filepath)\n",
    "eff_rate.rename(columns={'date': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "er_condos = pd.concat([condos, eff_rate], axis=1, join='inner')\n",
    "er_condos.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfffc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = er_condos.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig('hvalues_vs_ER.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b15ce7",
   "metadata": {},
   "source": [
    "## CPI vs Condo Prices Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f615301",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('CPI.csv')\n",
    "cpi = pd.read_csv(filepath)\n",
    "cpi.rename(columns={'date': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_condos = pd.concat([condos, cpi], axis=1, join='inner')\n",
    "cpi_condos.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9570b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = cpi_condos.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig('cpi_v_homeprices.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f793d",
   "metadata": {},
   "source": [
    "## Percentage Gains by Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98903863",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow extra/Zip_Code_SFR-ONLY_month.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853e3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df1 = pd.read_csv(home_path)\n",
    "df1 = df1.drop(['SizeRank',\n",
    "                'RegionType',\n",
    "                'StateName',\n",
    "                'State',\n",
    "                'Metro',\n",
    "                'City',\n",
    "                'RegionName'\n",
    "               ],\n",
    "               axis=1)\n",
    "df1.columns = [\n",
    "    pd.to_datetime(col[0]) if isinstance(col, tuple) else col\n",
    "    for col in df1.columns\n",
    "]\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For export to Tableau\n",
    "# Define the date range for columns you want to drop\n",
    "start1 = '2000-02-29'\n",
    "end1 = '2007-02-28'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start1 <= col <= end1\n",
    "]\n",
    "pre_GFC = df1.drop(df1.columns[drop_columns_indices], axis=1)\n",
    "\n",
    "# Drop rest of the date range post-GFC\n",
    "start2 = '2007-03-31'\n",
    "# Find the index of the starting date column\n",
    "idx = pre_GFC.columns.get_loc(start2)\n",
    "\n",
    "columns_to_drop = pre_GFC.columns[idx + 1:]\n",
    "\n",
    "pre_GFC = pre_GFC.drop(columns_to_drop, axis=1)\n",
    "pre_GFC = pre_GFC.dropna()\n",
    "pre_GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ae5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "pre_GFC['Percent Gain'] = (pre_GFC['2007-03-31'] / pre_GFC['2000-01-31'] - 1) * 100\n",
    "pre_GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beacf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC.to_csv('tableau_Pre-GFC.csv', index=False)\n",
    "dfp = df1.rename(columns={'RegionID' : 'Date'})\n",
    "dfp = dfp.drop(['CountyName'], axis=1)\n",
    "# Transpose while setting index to Date (this DF is for python, the rest are for export to Tableau)\n",
    "dft = dfp.set_index('Date').T\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start3 = '2007-04-30'\n",
    "end3 = '2012-02-29'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices3 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start3 <= col <= end3\n",
    "]\n",
    "GFC = df1.drop(df1.columns[drop_columns_indices3], axis=1)\n",
    "# Drop everything after the date range\n",
    "start4 = '2012-03-31'\n",
    "# Find the index of the starting date column\n",
    "idx3 = GFC.columns.get_loc(start4)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop3 = GFC.columns[idx3 + 1:]\n",
    "GFC = GFC.drop(columns_to_drop3, axis=1)\n",
    "# Drop everything before the date range\n",
    "end4 = '2007-03-31'\n",
    "idx4 = GFC.columns.get_loc(end4)\n",
    "columns_to_drop4 = GFC.columns[2:idx4]\n",
    "GFC = GFC.drop(columns_to_drop4, axis=1)\n",
    "\n",
    "GFC = GFC.dropna()\n",
    "GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4497f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "GFC['Percent Gain'] = (GFC['2012-03-31'] / GFC['2007-03-31'] - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ef808",
   "metadata": {},
   "outputs": [],
   "source": [
    "GFC.to_csv('tableau_GFC.csv', index=False)\n",
    "GFC_sorted = GFC.sort_values(by='Percent Gain', ascending=False)\n",
    "GFC_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76216681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start5 = '2012-04-30'\n",
    "end5 = '2022-05-31'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices5 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start5 <= col <= end5\n",
    "]\n",
    "QE = df1.drop(df1.columns[drop_columns_indices5], axis=1)\n",
    "# Drop everything after the date range\n",
    "start6 = '2022-06-30'\n",
    "# Find the index of the starting date column\n",
    "idx5 = QE.columns.get_loc(start6)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop5 = QE.columns[idx5 + 1:]\n",
    "QE = QE.drop(columns_to_drop5, axis=1)\n",
    "\n",
    "# Drop everything before the date range\n",
    "end6 = '2012-03-31'\n",
    "idx6 = QE.columns.get_loc(end6)\n",
    "columns_to_drop6 = QE.columns[2:idx6]\n",
    "QE = QE.drop(columns_to_drop6, axis=1)\n",
    "\n",
    "QE = QE.dropna()\n",
    "QE.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "QE['Percent Gain'] = (QE['2022-06-30'] / QE['2012-03-31'] - 1) * 100\n",
    "QE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c352387",
   "metadata": {},
   "outputs": [],
   "source": [
    "QE.to_csv('tableau_QE.csv', index=False)\n",
    "QE_sorted = QE.sort_values(by='Percent Gain', ascending=False)\n",
    "QE_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start7 = '2022-07-31'\n",
    "end7 = '2023-01-31'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices7 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start7 <= col <= end7\n",
    "]\n",
    "QT = df1.drop(df1.columns[drop_columns_indices7], axis=1)\n",
    "\n",
    "# Drop everything after the date range\n",
    "start8 = '2023-02-28'\n",
    "# Find the index of the starting date column\n",
    "idx7 = QT.columns.get_loc(start8)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop7 = QT.columns[idx7 + 1:]\n",
    "QT = QT.drop(columns_to_drop7, axis=1)\n",
    "\n",
    "# Drop everything before the date range\n",
    "end8 = '2022-06-30'\n",
    "idx8 = QT.columns.get_loc(end8)\n",
    "columns_to_drop8 = QT.columns[2:idx8]\n",
    "QT = QT.drop(columns_to_drop8, axis=1)\n",
    "QT = QT.dropna()\n",
    "QT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31fdf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "QT['Percent Gain'] = (QT['2023-02-28'] / QT['2022-06-30'] - 1) * 100\n",
    "QT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "QT_sorted = QT.sort_values(by='Percent Gain', ascending=True)\n",
    "QT_sorted.head()\n",
    "QT.to_csv('tableau_QT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cd2bd",
   "metadata": {},
   "source": [
    "## Rates vs Fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_path = 'effective_rate.csv'\n",
    "fed_path = 'total_assets.csv'\n",
    "cpi_path = 'CPI.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = pd.read_csv(rate_path)\n",
    "fed = pd.read_csv(fed_path)\n",
    "cpi = pd.read_csv(cpi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi['date'] = pd.to_datetime(cpi['date'])\n",
    "cpi.set_index('date', inplace=True)\n",
    "cpi.tail\n",
    "cpi['CPI_yr'] = cpi['CPI'].diff(periods=12)\n",
    "cpi = cpi.drop(columns=['CPI', 'CPI_change'])\n",
    "cpi.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e28b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "plt.plot(cpi.index, cpi['CPI_yr'], label='CPI')\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('CPI')\n",
    "plt.title('Monthly CPI')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0143c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate['date'] = pd.to_datetime(rate['date'])\n",
    "fed['date'] = pd.to_datetime(fed['date'])\n",
    "rate.set_index('date', inplace=True)\n",
    "fed.set_index('date', inplace=True)\n",
    "concat = rate.merge(fed, on='date', how='outer')\n",
    "concat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c917ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "three = concat.merge(cpi, on='date', how='inner')\n",
    "norm_three = (three - three.min()) / (three.max() - three.min())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "plt.plot(norm_three.index, norm_three['CPI_yr'], c='r', label='Annual CPI')\n",
    "plt.plot(norm_three.index, norm_three['total_assets'], label='Fed_Total_Assets')\n",
    "plt.plot(norm_three.index, norm_three['effective_rate'], label='Effective_Rate')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Interest Rates, CPI, and the Fed Balance Sheet Normalized')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec3ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 9))\n",
    "# Line plot for Asset1_Price\n",
    "plt.plot(norm.index, norm['total_assets'], label='Fed_Total_Assets')\n",
    "\n",
    "# Line plot for Asset2_Price\n",
    "plt.plot(norm.index, norm['effective_rate'], label='effective_rate')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Interest Rates vs Fed Balance Sheet Normalized')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eddc06",
   "metadata": {},
   "source": [
    "## Zillow Cities Home Values vs  Interest Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = Path('../Resources/FRED corrected dates/effective_rate.csv')\n",
    "rate = pd.read_csv(rpath)\n",
    "rate['date'] = pd.to_datetime(rate['date'])\n",
    "rcol = ['Date', 'Effective_Rate']\n",
    "rate.columns = rcol\n",
    "cpath = '../Resources/Zillow Cleaned up/city_condo_smoothed_by_date.csv'\n",
    "cities = pd.read_csv(cpath)\n",
    "cities['date'] = pd.to_datetime(cities['date'])\n",
    "# Create a subset of DataFrame with specific columns using iloc\n",
    "columns_to_select = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "cities_df = cities.iloc[:, columns_to_select]\n",
    "# rename columns\n",
    "colum = ['Date', 'New York', 'Los Angeles', 'Houston', 'Chicago', 'San Antonio', 'Philadelphia', 'Phoenix', 'Las Vegas', 'San Diego', 'Dallas']\n",
    "cities_df.columns = colum\n",
    "one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "cities_df['Date'] = cities_df['Date'] + one_day # Add one day\n",
    "cities_df = cities_df.set_index('Date')\n",
    "\n",
    "rate['Date'] = rate['Date'] + pd.DateOffset(months=36) # Add x months to offset interest rate effect\n",
    "rate = rate.set_index('Date')\n",
    "cvr = pd.concat([rate, cities_df], axis='columns', join='inner')\n",
    "price_correlation = cvr.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f7083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Line plot for Normalized Asset Price of each column\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "plt.plot(cvr_norm.index, cvr_norm['Effective_Rate'], linestyle=(0, (5, 2)), label='Effective_Rate')\n",
    "plt.plot(cvr_norm.index, cvr_norm['Los Angeles'], label='Los Angeles')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Normalized Monthly Home Values Compared to Fed Balance Sheet')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.scatter(cvr_norm.index, cvr_norm['Effective_Rate'], label='Effective_Rate')\n",
    "plt.scatter(cvr_norm.index, cvr_norm['Los Angeles'], label='Los Angeles')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Scatter Plot of Top and Bottom Tier homes in six cities vs Fed Balance Sheet')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d425eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "plt.plot(cvr.index, cvr['Effective_Rate'], linestyle=(0, (5, 2)), label='Effective_Rate')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Normalized Monthly Home Values Compared to Fed Balance Sheet')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4eb75f",
   "metadata": {},
   "source": [
    "## Zillow Cities Home Values vs Fed Balance Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c67b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottompath = '../Resources/Zillow Cleaned Up/bottom_tier_by_date.csv'\n",
    "toppath = '../Resources/Zillow Cleaned Up/top_tier_by_date.csv'\n",
    "path = '../Resources/City_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv'\n",
    "bottom = pd.read_csv(bottompath)\n",
    "top = pd.read_csv(toppath)\n",
    "City_zhvi = pd.read_csv(path)\n",
    "City_zhvi.rename(columns={'RegionName' : 'Date'}, inplace = True)\n",
    "City_zhvi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smol = City_zhvi.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smol = Smol.transpose()\n",
    "Smol = Smol.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc95a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Smol['index'] = pd.to_datetime(Smol['index'])\n",
    "# How to add 1 day with timedelta\n",
    "# df['index'] = df['index'] + pd.Timedelta(days=1)\n",
    "Smol.to_csv('smol_test.csv', index=True)\n",
    "columns1 = ['index', 'Chicago', 'Matamoras', 'Richburg', 'Haileyville', 'Peterson', 'Surfside Beach', 'Mount Charleston', 'Clearview', 'Tribes Hill', 'Haskins', 'Galesville']\n",
    "cities_df = Smol[columns1]\n",
    "cities_df.head()\n",
    "# Create a subset of DataFrame using iloc (not using)\n",
    "# columns2 = [0, 5, 6, 7, 8, 13, 14, 15, 16, 17, 18]\n",
    "# cities_df = cities_df.iloc[:, columns2]\n",
    "# rename columns\n",
    "colum = ['Date', 'Chicago', 'Matamoras', 'Richburg', 'Haileyville', 'Peterson', 'Surfside Beach', 'Mount Charleston', 'Clearview', 'Tribes Hill', 'Haskins', 'Galesville']\n",
    "cities_df.columns = colum\n",
    "cities_df.set_index('Date', inplace=True)\n",
    "cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom = bottom.dropna(axis=1)\n",
    "top = top.dropna(axis=1)\n",
    "tcol = ['date', 'Las Vegas, NV', 'Prineville, OR', 'Columbus, NE', 'Washington Court House, OH', 'Camden, AR', 'Brookings, OR', 'Sterling, CO', 'Forrest City, AR', 'Cordele, GA', 'Fitzgerald, GA', 'Craig, CO']\n",
    "top_df = top[tcol]\n",
    "renamet = ['Date', 'Las Vegas, NV', 'Prineville, OR', 'Columbus, NE', 'Washington Court House, OH', 'Camden, AR', 'Brookings, OR', 'Sterling, CO', 'Forrest City, AR', 'Cordele, GA', 'Fitzgerald, GA', 'Craig, CO']\n",
    "top_df.columns = renamet\n",
    "top_df['Date'] = pd.to_datetime(top_df['Date'])\n",
    "top_df = top_df.set_index('Date')\n",
    "top_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcol = ['date', 'Las Vegas, NV', 'Prineville, OR', 'Columbus, NE', 'Washington Court House, OH', 'Camden, AR', 'Brookings, OR', 'Sterling, CO', 'Forrest City, AR', 'Cordele, GA', 'Fitzgerald, GA', 'Craig, CO']\n",
    "bottom_df = bottom[bcol]\n",
    "renameb = ['Date', 'Las Vegas, NV', 'Prineville, OR', 'Columbus, NE', 'Washington Court House, OH', 'Camden, AR', 'Brookings, OR', 'Sterling, CO', 'Forrest City, AR', 'Cordele, GA', 'Fitzgerald, GA', 'Craig, CO']\n",
    "bottom_df.columns = renameb\n",
    "bottom_df['Date'] = pd.to_datetime(bottom_df['Date'])\n",
    "bottom_df = bottom_df.set_index('Date')\n",
    "bottom_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d321d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_balance = Path('../../total_assets.csv')\n",
    "fed_df = pd.read_csv(fed_balance)\n",
    "fed_df['date'] = pd.to_datetime(fed_df['date'])\n",
    "fed_c = ['Date', 'Total_Assets']\n",
    "fed_df.columns = fed_c\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "fed_df['Date'] = fed_df['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "fed_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate\n",
    "bottomfed = fed_df.merge(bottom_df, on='Date', how='inner')\n",
    "bottomfed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topfed = fed_df.merge(top_df, on='Date', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a460e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['date', 'Las Vegas, NV', 'Prineville, OR', 'Camden, AR', 'Washington Court House, OH', 'Camden, AR', 'Brookings, OR',\n",
    "# 'Sterling, CO', 'Forrest City, AR', 'Cordele, GA', 'Fitzgerald, GA', 'Craig, CO']\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Prineville, OR'], marker='.', c='m', label='Top Tier Prineville, OR (pop 9,200)')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Prineville, OR'], marker='x', c='m', label='Bottom Tier Prineville, OR')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Brookings, OR'], marker='.', c='r', label='Top Tier Brookings, OR (pop 6700)')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Brookings, OR'], marker='x', c='r', label='Bottom Tier Brookings, OR')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Las Vegas, NV'], marker='.', c='orange', label='Top Tier Las Vegas, NV')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Las Vegas, NV'], marker='x', c='orange', label='Bottom Tier Las Vegas, NV')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Columbus, NE'], marker='.', c='y', label='Top Tier Columbus, NE')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Columbus, NE'], marker='x', c='y', label='Bottom Tier Columbus, NE')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Washington Court House, OH'], marker='.', c='g', label='Top Tier Washington Court House, OH')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Washington Court House, OH'], marker='x', c='g', label='Bottom Tier Washington Court House, OH')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Camden, AR'], marker='.', c='b', label='Top Tier Camden, AR (pop 12,000)')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Camden, AR'], marker='x', c='b', label='Bottom Tier Camden, AR')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Fed Total Assets')\n",
    "plt.ylabel('Home Values')\n",
    "plt.title('Small town Home Values vs Fed Balance Sheet')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Las Vegas, NV'], marker='.', c='m', label='Top Tier Las Vegas, NV')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Las Vegas, NV'], marker='x', c='m', label='Bottom Tier Las Vegas, NV')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Sterling, CO'], marker='.', c='r', label='Top Tier Sterling, CO')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Sterling, CO'], marker='x', c='r', label='Bottom Tier Sterling, CO')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Craig, CO'], marker='.', c='orange', label='Top Tier Craig, CO')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Craig, CO'], marker='x', c='orange', label='Bottom Tier Craig, CO')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Cordele, GA'], marker='.', c='y', label='Top Tier Cordele, GA')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Cordele, GA'], marker='x', c='y', label='Bottom Tier Cordele, GA')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Fitzgerald, GA'], marker='.', c='g', label='Top Tier Fitzgerald, GA')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Fitzgerald, GA'], marker='x', c='g', label='Bottom Tier Fitzgerald, GA')\n",
    "plt.scatter(topfed['Total_Assets'], topfed['Forrest City, AR'], marker='.', c='b', label='Top Tier Forrest City, AR')\n",
    "plt.scatter(bottomfed['Total_Assets'], bottomfed['Forrest City, AR'], marker='x', c='b', label='Bottom Tier Forrest City, AR')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Fed Total Assets')\n",
    "plt.ylabel('Home Values')\n",
    "plt.title('Small town Home Values vs Fed Balance Sheet')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Chicago'], marker='x', c='m', label='Chicago')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Matamoras'], marker='.', c='r', label='Matamoras')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Richburg'], marker='.', c='orange', label='Richburg')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Haileyville'], marker='.', c='y', label='Haileyville')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Peterson'], marker='.', c='lime', label='Peterson')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Surfside Beach'], marker='.', c='green', label='Surfside Beach')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Mount Charleston'], marker='.', c='teal', label='Mount Charleston')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Clearview'], marker='.', c='b', label='Clearview')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Tribes Hill'], marker='.', c='purple', label='Tribes Hill')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Haskins'], marker='.', c='hotpink', label='Haskins')\n",
    "plt.scatter(small_towns['Total_Assets'], small_towns['Galesville'], marker='.', c='k', label='Galesville')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Fed Total Assets')\n",
    "plt.ylabel('Home Values')\n",
    "plt.title('Small town Home Values vs Fed Balance Sheet')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of raw prices\n",
    "price_correlation = small_towns.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a823763",
   "metadata": {},
   "source": [
    "## Zillow Metro 1&2BR vs fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow Cleaned up/one_bedroom_by_date.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'\n",
    "print(home_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc15467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat(home_path, fed_path):\n",
    "    df1 = pd.read_csv(home_path)\n",
    "    df2 = pd.read_csv(fed_path)\n",
    "    \n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1 = df1[columns_to_keep]\n",
    "    renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1.columns = renamed\n",
    "    df1.set_index('Date', inplace=True)\n",
    "    \n",
    "    df2['date'] = pd.to_datetime(df2['date'])\n",
    "    fed_c = ['Date', 'Total_Assets']\n",
    "    df2.columns = fed_c\n",
    "    \n",
    "    one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "    df2['Date'] = df2['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "    df2.set_index('Date', inplace=True)\n",
    "    \n",
    "    concat = pd.concat([df2, df1], axis='columns', join='inner')\n",
    "    return concat\n",
    "onebr = read_concat(home_path, fed_path)\n",
    "onebr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn(df, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    price_correlation = df.corr()\n",
    "    sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.savefig(name, dpi=300)\n",
    "    \n",
    "seaborn(onebr, 'seaborn Metro 1BR vs Fed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow Cleaned up/two_bedroom_by_date.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat(home_path, fed_path):\n",
    "    df1 = pd.read_csv(home_path)\n",
    "    df2 = pd.read_csv(fed_path)\n",
    "    \n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1 = df1[columns_to_keep]\n",
    "    renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1.columns = renamed\n",
    "    df1.set_index('Date', inplace=True)\n",
    "    \n",
    "    df2['date'] = pd.to_datetime(df2['date'])\n",
    "    fed_c = ['Date', 'Total_Assets']\n",
    "    df2.columns = fed_c\n",
    "    \n",
    "    one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "    df2['Date'] = df2['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "    df2.set_index('Date', inplace=True)\n",
    "    \n",
    "    concat = pd.concat([df2, df1], axis='columns', join='inner')\n",
    "    return concat\n",
    "twobr = read_concat(home_path, fed_path)\n",
    "twobr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn(df, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    price_correlation = df.corr()\n",
    "    sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.savefig(name, dpi=300)\n",
    "    \n",
    "seaborn(twobr, 'seaborn Metro 2BR vs Fed.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890abc8a",
   "metadata": {},
   "source": [
    "## Zillow Metro 3BR vs fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow Cleaned up/three_bedroom_by_date.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat(home_path, fed_path):\n",
    "    df1 = pd.read_csv(home_path)\n",
    "    df2 = pd.read_csv(fed_path)\n",
    "    \n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1 = df1[columns_to_keep]\n",
    "    renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1.columns = renamed\n",
    "    df1.set_index('Date', inplace=True)\n",
    "    \n",
    "    df2['date'] = pd.to_datetime(df2['date'])\n",
    "    fed_c = ['Date', 'Total_Assets']\n",
    "    df2.columns = fed_c\n",
    "    \n",
    "    one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "    df2['Date'] = df2['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "    df2.set_index('Date', inplace=True)\n",
    "    \n",
    "    concat = pd.concat([df2, df1], axis='columns', join='inner')\n",
    "    return concat\n",
    "threebr = read_concat(home_path, fed_path)\n",
    "threebr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn(df, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    price_correlation = df.corr()\n",
    "    sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.savefig(name, dpi=300)\n",
    "    \n",
    "seaborn(threebr, 'seaborn Metro 3BR vs Fed.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c377202c",
   "metadata": {},
   "source": [
    "## Zillow Metro 4BR vs fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215dd992",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow Cleaned up/four_bedroom_by_date.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_concat(home_path, fed_path):\n",
    "    df1 = pd.read_csv(home_path)\n",
    "    df2 = pd.read_csv(fed_path)\n",
    "    \n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1 = df1[columns_to_keep]\n",
    "    renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1.columns = renamed\n",
    "    df1.set_index('Date', inplace=True)\n",
    "    \n",
    "    df2['date'] = pd.to_datetime(df2['date'])\n",
    "    fed_c = ['Date', 'Total_Assets']\n",
    "    df2.columns = fed_c\n",
    "    \n",
    "    one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "    df2['Date'] = df2['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "    df2.set_index('Date', inplace=True)\n",
    "    \n",
    "    concat = pd.concat([df2, df1], axis='columns', join='inner')\n",
    "    return concat\n",
    "fourbr = read_concat(home_path, fed_path)\n",
    "fourbr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b91c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn(df, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    price_correlation = df.corr()\n",
    "    sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.savefig(name, dpi=300)\n",
    "    \n",
    "seaborn(fourbr, 'seaborn Metro 4BR vs Fed.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442ee99",
   "metadata": {},
   "source": [
    "## Zillow Metro 5BR vs fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Resources/Zillow Cleaned up/five_bedroom_by_date.csv'\n",
    "fivebrraw = pd.read_csv(path)\n",
    "fivebrraw['date'] = pd.to_datetime(fivebrraw['date'])\n",
    "columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "fivebr = fivebrraw[columns_to_keep]\n",
    "renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "fivebr.columns = renamed\n",
    "fivebr.set_index('Date', inplace=True)\n",
    "fivebr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_balance = Path('../Resources/FRED corrected dates/total_assets.csv')\n",
    "fed_df = pd.read_csv(fed_balance)\n",
    "fed_df['date'] = pd.to_datetime(fed_df['date'])\n",
    "fed_c = ['Date', 'Total_Assets']\n",
    "fed_df.columns = fed_c\n",
    "one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "fed_df['Date'] = fed_df['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "fed_df.set_index('Date', inplace=True)\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbr_v_fed = pd.concat([fed_df, fivebr], axis='columns', join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bbf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "price_correlation = fbr_v_fed.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.savefig('seaborn Metro 5BR vs Fed.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0269aee",
   "metadata": {},
   "source": [
    "## Zillow Metro Bottom Tier vs fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow Cleaned up/bottom_tier_by_date.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'\n",
    "def read_concat(home_path, fed_path):\n",
    "    df1 = pd.read_csv(home_path)\n",
    "    df2 = pd.read_csv(fed_path)\n",
    "    \n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1 = df1[columns_to_keep]\n",
    "    renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1.columns = renamed\n",
    "    df1.set_index('Date', inplace=True)\n",
    "    \n",
    "    df2['date'] = pd.to_datetime(df2['date'])\n",
    "    fed_c = ['Date', 'Total_Assets']\n",
    "    df2.columns = fed_c\n",
    "    \n",
    "    one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "    df2['Date'] = df2['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "    df2.set_index('Date', inplace=True)\n",
    "    \n",
    "    concat = pd.concat([df2, df1], axis='columns', join='inner')\n",
    "    return concat\n",
    "bottom_tier = read_concat(home_path, fed_path)\n",
    "bottom_tier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71a457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn(df, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    price_correlation = df.corr()\n",
    "    sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.savefig(name, dpi=300)\n",
    "    \n",
    "seaborn(bottom_tier, 'seaborn Metro Bottom vs Fed.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccec56b",
   "metadata": {},
   "source": [
    "## Zillow Metro Top Tier vs fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e27b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow Cleaned up/top_tier_by_date.csv'\n",
    "fed_path = '../Resources/FRED corrected dates/total_assets.csv'\n",
    "def read_concat(home_path, fed_path):\n",
    "    df1 = pd.read_csv(home_path)\n",
    "    df2 = pd.read_csv(fed_path)\n",
    "    \n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    columns_to_keep = ['date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1 = df1[columns_to_keep]\n",
    "    renamed = ['Date', 'New York, NY', 'Los Angeles, CA', 'Houston, TX', 'Chicago, IL', 'San Antonio, TX', 'Philadelphia, PA', 'Phoenix, AZ', 'Las Vegas, NV', 'San Diego, CA', 'Dallas, TX']\n",
    "    df1.columns = renamed\n",
    "    df1.set_index('Date', inplace=True)\n",
    "    \n",
    "    df2['date'] = pd.to_datetime(df2['date'])\n",
    "    fed_c = ['Date', 'Total_Assets']\n",
    "    df2.columns = fed_c\n",
    "    \n",
    "    one_day = pd.Timedelta(days=1)  # Create a Timedelta representing one day\n",
    "    df2['Date'] = df2['Date'] - one_day # Subtract one day from fed balance sheet dates\n",
    "    df2.set_index('Date', inplace=True)\n",
    "    \n",
    "    concat = pd.concat([df2, df1], axis='columns', join='inner')\n",
    "    return concat\n",
    "top_tier = read_concat(home_path, fed_path)\n",
    "top_tier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4252db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn(df, name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    price_correlation = df.corr()\n",
    "    sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "    plt.savefig(name, dpi=300)\n",
    "    \n",
    "seaborn(top_tier, 'seaborn Metro top_tier vs Fed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "home2_path = '../Resources/Zillow Cleaned up/bottom_tier_by_date.csv'\n",
    "fed2_path = '../Resources/FRED corrected dates/total_assets.csv'\n",
    "bottom_tier = read_concat(home2_path, fed2_path)\n",
    "bottom_tier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.scatter(top_tier['Total_Assets'], top_tier['Los Angeles, CA'], marker='.', c='m', label='Top Tier Los Angeles')\n",
    "plt.scatter(bottom_tier['Total_Assets'], bottom_tier['Los Angeles, CA'], marker='x', c='m', label='Bottom Tier')\n",
    "plt.scatter(top_tier['Total_Assets'], top_tier['San Diego, CA'], marker='.', c='r', label='Top Tier San Diego')\n",
    "plt.scatter(bottom_tier['Total_Assets'], bottom_tier['San Diego, CA'], marker='x', c='r', label='Bottom Tier')\n",
    "plt.scatter(top_tier['Total_Assets'], top_tier['New York, NY'], marker='.', c='orange', label='Top Tier New York')\n",
    "plt.scatter(bottom_tier['Total_Assets'], bottom_tier['New York, NY'], marker='x', c='orange', label='Bottom Tier')\n",
    "plt.scatter(top_tier['Total_Assets'], top_tier['Las Vegas, NV'], marker='.', c='y', label='Top Tier Las Vegas')\n",
    "plt.scatter(bottom_tier['Total_Assets'], bottom_tier['Las Vegas, NV'], marker='x', c='y', label='Bottom Tier')\n",
    "plt.scatter(top_tier['Total_Assets'], top_tier['Houston, TX'], marker='.', c='lime', label='Top Tier Houston')\n",
    "plt.scatter(bottom_tier['Total_Assets'], bottom_tier['Houston, TX'], marker='x', c='lime', label='Bottom Tier')\n",
    "plt.scatter(top_tier['Total_Assets'], top_tier['Chicago, IL'], marker='.', c='teal', label='Top Tier Chicago')\n",
    "plt.scatter(bottom_tier['Total_Assets'], bottom_tier['Chicago, IL'], marker='x', c='teal', label='Bottom Tier')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Fed Total Assets')\n",
    "plt.ylabel('Home Values')\n",
    "plt.title('Scatter Plot of Top and Bottom Tier homes in six cities vs Fed Balance Sheet')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0.1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049c453",
   "metadata": {},
   "source": [
    "## Zip code Income vs nominal gains by Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow extra/Zip_Code_SFR-ONLY_month.csv'\n",
    "irs_path = '../Resources/Income/IRS_income_2019.csv'\n",
    "# Data cleaning\n",
    "df = pd.read_csv(irs_path)\n",
    "irs = df[['zipcode', 'N1', 'A00100']]\n",
    "irs.columns=['zip', 'returns', 'AGI']\n",
    "irs2 = irs.set_index(['zip'])\n",
    "irs2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f546ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the average income for each tax bracket in each zip code\n",
    "irs2['Average_Income'] = (irs2['AGI'] * 1000) / irs2['returns']\n",
    "# Step 2: Group by 'zip' and calculate the mean of 'Average_Income' for each zip code\n",
    "average_income_by_zip = irs2.groupby(irs2.index)['Average_Income'].mean()\n",
    "irs3 = pd.DataFrame(average_income_by_zip)\n",
    "irs3.columns = ['Avg_AGI']\n",
    "irs3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525313dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df1 = pd.read_csv(home_path)\n",
    "df1 = df1.drop(['SizeRank',\n",
    "                'RegionType',\n",
    "                'StateName',\n",
    "                'State',\n",
    "                'Metro',\n",
    "                'City',\n",
    "                'RegionName'\n",
    "               ],\n",
    "               axis=1)\n",
    "df1.columns = [\n",
    "    pd.to_datetime(col[0]) if isinstance(col, tuple) else col\n",
    "    for col in df1.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00547a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For export to Tableau\n",
    "# Define the date range for columns you want to drop\n",
    "start1 = '2000-02-29'\n",
    "end1 = '2007-02-28'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start1 <= col <= end1\n",
    "]\n",
    "pre_GFC = df1.drop(df1.columns[drop_columns_indices], axis=1)\n",
    "\n",
    "# Drop rest of the date range post-GFC\n",
    "start2 = '2007-03-31'\n",
    "# Find the index of the starting date column\n",
    "idx = pre_GFC.columns.get_loc(start2)\n",
    "\n",
    "columns_to_drop = pre_GFC.columns[idx + 1:]\n",
    "\n",
    "pre_GFC = pre_GFC.drop(columns_to_drop, axis=1)\n",
    "pre_GFC = pre_GFC.dropna()\n",
    "pre_GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5627a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC = pre_GFC.drop(['CountyName'], axis=1)\n",
    "pre_GFC.columns = ['zip', '2000-01-31', '2007-03-31']\n",
    "pre_GFC = pre_GFC.set_index(['zip'])\n",
    "pre_GFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate nominal gain\n",
    "pre_GFC['Gain'] = (pre_GFC['2007-03-31'] - pre_GFC['2000-01-31'])\n",
    "pre_GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC = pre_GFC.drop(columns=['2000-01-31', '2007-03-31'])\n",
    "pre_GFC_IRS = pd.merge(pre_GFC, irs3, on='zip', how='inner')\n",
    "pre_GFC_IRS = pre_GFC_IRS.dropna()\n",
    "# Calculate the lower and upper quantile values\n",
    "q_low = pre_GFC_IRS.quantile(0.05)\n",
    "q_high = pre_GFC_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "pre_GFC_IRS = pre_GFC_IRS[(pre_GFC_IRS >= q_low) & (pre_GFC_IRS <= q_high)]\n",
    "pre_GFC_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC_IRS = pre_GFC_IRS.dropna()\n",
    "pre_GFC_IRS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06032dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC_IRS = pre_GFC_IRS.drop(index=pre_GFC_IRS[pre_GFC_IRS['Avg_AGI'] == 0].index)\n",
    "pre_GFC_IRS.to_csv('test_irs.csv', index=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'lightgreen'), (1, 'green')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "plt.scatter(pre_GFC_IRS['Avg_AGI'], pre_GFC_IRS['Gain'], c=pre_GFC_IRS['Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Gain/Loss')\n",
    "plt.title('Income (2019) vs. Home Value Gain (2000-2007) by Zip Code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79161400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start3 = '2007-04-30'\n",
    "end3 = '2012-02-29'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices3 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start3 <= col <= end3\n",
    "]\n",
    "GFC = df1.drop(df1.columns[drop_columns_indices3], axis=1)\n",
    "# Drop everything after the date range\n",
    "start4 = '2012-03-31'\n",
    "# Find the index of the starting date column\n",
    "idx3 = GFC.columns.get_loc(start4)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop3 = GFC.columns[idx3 + 1:]\n",
    "GFC = GFC.drop(columns_to_drop3, axis=1)\n",
    "# Drop everything before the date range\n",
    "end4 = '2007-03-31'\n",
    "idx4 = GFC.columns.get_loc(end4)\n",
    "columns_to_drop4 = GFC.columns[2:idx4]\n",
    "GFC = GFC.drop(columns_to_drop4, axis=1)\n",
    "\n",
    "GFC = GFC.dropna()\n",
    "GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596799e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "GFC['Gain'] = (GFC['2012-03-31'] - GFC['2007-03-31'])\n",
    "# GFC.to_csv('tableau_GFC.csv', index=False)\n",
    "GFC = GFC.drop(columns=['CountyName', '2007-03-31', '2012-03-31'], axis=1)\n",
    "GFC.columns = ['zip', 'Gain']\n",
    "GFC = GFC.set_index(['zip'])\n",
    "GFC_IRS = pd.merge(GFC, irs3, on='zip', how='inner')\n",
    "GFC_IRS = GFC_IRS.dropna()\n",
    "GFC_IRS.isnull().sum()\n",
    "GFC_IRS.columns = ['Gain', 'Avg_AGI']\n",
    "GFC_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea980fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantile values\n",
    "gfcq_low = GFC_IRS.quantile(0.05)\n",
    "gfcq_high = GFC_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "GFC_IRS = GFC_IRS[(GFC_IRS >= gfcq_low) & (GFC_IRS <= gfcq_high)]\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'darkred'), (1, 'darkgoldenrod')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "plt.scatter(GFC_IRS['Avg_AGI'], GFC_IRS['Gain'], c=GFC_IRS['Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Gain/Loss')\n",
    "plt.title('Income (2019) vs. Home Value Gain/Loss (2007-2012) by zip code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b962750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start5 = '2012-04-30'\n",
    "end5 = '2022-05-31'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices5 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start5 <= col <= end5\n",
    "]\n",
    "QE = df1.drop(df1.columns[drop_columns_indices5], axis=1)\n",
    "# Drop everything after the date range\n",
    "start6 = '2022-06-30'\n",
    "# Find the index of the starting date column\n",
    "idx5 = QE.columns.get_loc(start6)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop5 = QE.columns[idx5 + 1:]\n",
    "QE = QE.drop(columns_to_drop5, axis=1)\n",
    "\n",
    "# Drop everything before the date range\n",
    "end6 = '2012-03-31'\n",
    "idx6 = QE.columns.get_loc(end6)\n",
    "columns_to_drop6 = QE.columns[2:idx6]\n",
    "QE = QE.drop(columns_to_drop6, axis=1)\n",
    "\n",
    "QE = QE.dropna()\n",
    "QE.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "QE['Gain'] = (QE['2022-06-30'] - QE['2012-03-31'])\n",
    "QE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "QE = QE.drop(columns=['CountyName', '2012-03-31', '2022-06-30'], axis=1)\n",
    "QE.columns = ['zip', 'Gain']\n",
    "QE = QE.set_index(['zip'])\n",
    "QE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QE_IRS = pd.merge(QE, irs3, on='zip', how='inner')\n",
    "QE_IRS = QE_IRS.dropna()\n",
    "QE_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f569b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantile values\n",
    "qeq_low = QE_IRS.quantile(0.05)\n",
    "qeq_high = QE_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "QE_IRS = QE_IRS[(QE_IRS >= qeq_low) & (QE_IRS <= qeq_high)]\n",
    "QE_IRS = QE_IRS.dropna()\n",
    "QE_IRS.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a753997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'lightgreen'), (1, 'green')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "\n",
    "plt.scatter(QE_IRS['Avg_AGI'], QE_IRS['Gain'], c=QE_IRS['Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Gain')\n",
    "plt.title('Income (2019) vs. Home Value Gain (2012-2022) by zip code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start7 = '2022-07-31'\n",
    "end7 = '2023-01-31'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices7 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start7 <= col <= end7\n",
    "]\n",
    "QT = df1.drop(df1.columns[drop_columns_indices7], axis=1)\n",
    "\n",
    "# Drop everything after the date range\n",
    "start8 = '2023-02-28'\n",
    "# Find the index of the starting date column\n",
    "idx7 = QT.columns.get_loc(start8)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop7 = QT.columns[idx7 + 1:]\n",
    "QT = QT.drop(columns_to_drop7, axis=1)\n",
    "\n",
    "# Drop everything before the date range\n",
    "end8 = '2022-06-30'\n",
    "idx8 = QT.columns.get_loc(end8)\n",
    "columns_to_drop8 = QT.columns[2:idx8]\n",
    "QT = QT.drop(columns_to_drop8, axis=1)\n",
    "QT = QT.dropna()\n",
    "QT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cadae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "QT['Gain'] = (QT['2023-02-28'] - QT['2022-06-30'])\n",
    "QT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "QT = QT.drop(columns=['CountyName', '2022-06-30', '2023-02-28'], axis=1)\n",
    "QT.columns = ['zip', 'Percent Gain']\n",
    "QT = QT.set_index(['zip'])\n",
    "QT_IRS = pd.merge(QT, irs3, on='zip', how='inner')\n",
    "QT_IRS = QT_IRS.dropna()\n",
    "QT_IRS.isnull().sum()\n",
    "QT_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantile values\n",
    "qtq_low = QT_IRS.quantile(0.05)\n",
    "qtq_high = QT_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "QT_IRS = QT_IRS[(QT_IRS >= qtq_low) & (QT_IRS <= qtq_high)]\n",
    "\n",
    "QT_IRS.columns = ['Gain', 'Avg_AGI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3fc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'red'), (1, 'green')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "\n",
    "plt.scatter(QT_IRS['Avg_AGI'], QT_IRS['Gain'], c=QT_IRS['Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Gain')\n",
    "plt.title('Income (2019) vs. Home Value Gain/Loss (2022-2023) by Zip Code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c132352",
   "metadata": {},
   "outputs": [],
   "source": [
    "QT_sorted = QT.sort_values(by='Percent Gain', ascending=True)\n",
    "QT.to_csv('tableau_QT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e9888",
   "metadata": {},
   "source": [
    "## Zip code Income vs percentage gains by Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = '../Resources/Zillow extra/Zip_Code_SFR-ONLY_month.csv'\n",
    "irs_path = '../Resources/Income/IRS_income_2019.csv'\n",
    "# Data cleaning\n",
    "df = pd.read_csv(irs_path)\n",
    "irs = df[['zipcode', 'N1', 'A00100']]\n",
    "irs.columns=['zip', 'returns', 'AGI']\n",
    "irs2 = irs.set_index(['zip'])\n",
    "irs2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13963c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the average income for each tax bracket in each zip code\n",
    "irs2['Average_Income'] = (irs2['AGI'] * 1000) / irs2['returns']\n",
    "# Step 2: Group by 'zip' and calculate the mean of 'Average_Income' for each zip code\n",
    "average_income_by_zip = irs2.groupby(irs2.index)['Average_Income'].mean()\n",
    "irs3 = pd.DataFrame(average_income_by_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "irs3.to_csv('test_irs.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de2b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "irs3.columns = ['Avg_AGI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df1 = pd.read_csv(home_path)\n",
    "df1 = df1.drop(['SizeRank',\n",
    "                'RegionType',\n",
    "                'StateName',\n",
    "                'State',\n",
    "                'Metro',\n",
    "                'City',\n",
    "                'RegionName'\n",
    "               ],\n",
    "               axis=1)\n",
    "df1.columns = [\n",
    "    pd.to_datetime(col[0]) if isinstance(col, tuple) else col\n",
    "    for col in df1.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b412c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For export to Tableau\n",
    "# Define the date range for columns you want to drop\n",
    "start1 = '2000-02-29'\n",
    "end1 = '2007-02-28'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start1 <= col <= end1\n",
    "]\n",
    "pre_GFC = df1.drop(df1.columns[drop_columns_indices], axis=1)\n",
    "\n",
    "# Drop rest of the date range post-GFC\n",
    "start2 = '2007-03-31'\n",
    "# Find the index of the starting date column\n",
    "idx = pre_GFC.columns.get_loc(start2)\n",
    "\n",
    "columns_to_drop = pre_GFC.columns[idx + 1:]\n",
    "\n",
    "pre_GFC = pre_GFC.drop(columns_to_drop, axis=1)\n",
    "pre_GFC = pre_GFC.dropna()\n",
    "pre_GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC = pre_GFC.drop(['CountyName'], axis=1)\n",
    "pre_GFC.columns = ['zip', '2000-01-31', '2007-03-31']\n",
    "pre_GFC = pre_GFC.set_index(['zip'])\n",
    "pre_GFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "pre_GFC['Percent Gain'] = (pre_GFC['2007-03-31'] / pre_GFC['2000-01-31'] - 1) * 100\n",
    "pre_GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a811bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC = pre_GFC.drop(columns=['2000-01-31', '2007-03-31'])\n",
    "pre_GFC_IRS = pd.merge(pre_GFC, irs3, on='zip', how='inner')\n",
    "pre_GFC_IRS = pre_GFC_IRS.dropna()\n",
    "# Calculate the lower and upper quantile values\n",
    "q_low = pre_GFC_IRS.quantile(0.05)\n",
    "q_high = pre_GFC_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "pre_GFC_IRS = pre_GFC_IRS[(pre_GFC_IRS >= q_low) & (pre_GFC_IRS <= q_high)]\n",
    "pre_GFC_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_GFC_IRS = pre_GFC_IRS.drop(index=pre_GFC_IRS[pre_GFC_IRS['Avg_AGI'] == 0].index)\n",
    "pre_GFC_IRS.to_csv('test_irs.csv', index=True)\n",
    "pre_GFC_IRS = pre_GFC_IRS.dropna()\n",
    "pre_GFC_IRS.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'lightgreen'), (1, 'green')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "plt.scatter(pre_GFC_IRS['Avg_AGI'], pre_GFC_IRS['Percent Gain'], c=pre_GFC_IRS['Percent Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Percent Gain')\n",
    "plt.title('Income (2019) vs. Home Value Percentage Gain (2000-2007) by zip code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start3 = '2007-04-30'\n",
    "end3 = '2012-02-29'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices3 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start3 <= col <= end3\n",
    "]\n",
    "GFC = df1.drop(df1.columns[drop_columns_indices3], axis=1)\n",
    "# Drop everything after the date range\n",
    "start4 = '2012-03-31'\n",
    "# Find the index of the starting date column\n",
    "idx3 = GFC.columns.get_loc(start4)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop3 = GFC.columns[idx3 + 1:]\n",
    "GFC = GFC.drop(columns_to_drop3, axis=1)\n",
    "# Drop everything before the date range\n",
    "end4 = '2007-03-31'\n",
    "idx4 = GFC.columns.get_loc(end4)\n",
    "columns_to_drop4 = GFC.columns[2:idx4]\n",
    "GFC = GFC.drop(columns_to_drop4, axis=1)\n",
    "\n",
    "GFC = GFC.dropna()\n",
    "GFC.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "GFC['Percent Gain'] = (GFC['2012-03-31'] / GFC['2007-03-31'] - 1) * 100\n",
    "GFC.to_csv('tableau_GFC.csv', index=False)\n",
    "GFC = GFC.drop(columns=['CountyName', '2007-03-31', '2012-03-31'], axis=1)\n",
    "GFC.columns = ['zip', 'Percent Gain']\n",
    "GFC = GFC.set_index(['zip'])\n",
    "GFC_IRS = pd.merge(GFC, irs3, on='zip', how='inner')\n",
    "GFC_IRS = GFC_IRS.dropna()\n",
    "GFC_IRS.isnull().sum()\n",
    "GFC_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa866b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantile values\n",
    "gfcq_low = GFC_IRS.quantile(0.05)\n",
    "gfcq_high = GFC_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "GFC_IRS = GFC_IRS[(GFC_IRS >= gfcq_low) & (GFC_IRS <= gfcq_high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a023c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'darkred'), (1, 'darkgoldenrod')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "plt.scatter(GFC_IRS['Avg_AGI'], GFC_IRS['Percent Gain'], c=GFC_IRS['Percent Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Percent Gain')\n",
    "plt.title('Income (2019) vs. Home Value Percentage Gain (2007-2012) by zip code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start5 = '2012-04-30'\n",
    "end5 = '2022-05-31'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices5 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start5 <= col <= end5\n",
    "]\n",
    "QE = df1.drop(df1.columns[drop_columns_indices5], axis=1)\n",
    "# Drop everything after the date range\n",
    "start6 = '2022-06-30'\n",
    "# Find the index of the starting date column\n",
    "idx5 = QE.columns.get_loc(start6)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop5 = QE.columns[idx5 + 1:]\n",
    "QE = QE.drop(columns_to_drop5, axis=1)\n",
    "\n",
    "# Drop everything before the date range\n",
    "end6 = '2012-03-31'\n",
    "idx6 = QE.columns.get_loc(end6)\n",
    "columns_to_drop6 = QE.columns[2:idx6]\n",
    "QE = QE.drop(columns_to_drop6, axis=1)\n",
    "\n",
    "QE = QE.dropna()\n",
    "QE.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "QE['Percent Gain'] = (QE['2022-06-30'] / QE['2012-03-31'] - 1) * 100\n",
    "QE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6624f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QE = QE.drop(columns=['CountyName', '2012-03-31', '2022-06-30'], axis=1)\n",
    "QE.columns = ['zip', 'Percent Gain']\n",
    "QE = QE.set_index(['zip'])\n",
    "QE_IRS = pd.merge(QE, irs3, on='zip', how='inner')\n",
    "QE_IRS = QE_IRS.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantile values\n",
    "qeq_low = QE_IRS.quantile(0.05)\n",
    "qeq_high = QE_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "QE_IRS = QE_IRS[(QE_IRS >= qeq_low) & (QE_IRS <= qeq_high)]\n",
    "QE_IRS = QE_IRS.dropna()\n",
    "QE_IRS.isnull().sum()\n",
    "\n",
    "QE_IRS.to_csv('qe_irs_test.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ad8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'lightgreen'), (1, 'green')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "\n",
    "plt.scatter(QE_IRS['Avg_AGI'], QE_IRS['Percent Gain'], c=QE_IRS['Percent Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Percent Gain')\n",
    "plt.title('Income (2019) vs. Home Value Percentage Gain (2012-2022) by zip code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range for columns you want to drop\n",
    "start7 = '2022-07-31'\n",
    "end7 = '2023-01-31'\n",
    "# Find the indices of the columns to drop\n",
    "drop_columns_indices7 = [\n",
    "    idx for idx, col in enumerate(df1.columns)\n",
    "    if start7 <= col <= end7\n",
    "]\n",
    "QT = df1.drop(df1.columns[drop_columns_indices7], axis=1)\n",
    "\n",
    "# Drop everything after the date range\n",
    "start8 = '2023-02-28'\n",
    "# Find the index of the starting date column\n",
    "idx7 = QT.columns.get_loc(start8)\n",
    "# Drop columns from index to end\n",
    "columns_to_drop7 = QT.columns[idx7 + 1:]\n",
    "QT = QT.drop(columns_to_drop7, axis=1)\n",
    "\n",
    "# Drop everything before the date range\n",
    "end8 = '2022-06-30'\n",
    "idx8 = QT.columns.get_loc(end8)\n",
    "columns_to_drop8 = QT.columns[2:idx8]\n",
    "QT = QT.drop(columns_to_drop8, axis=1)\n",
    "QT = QT.dropna()\n",
    "QT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c244bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent gain and create a new 'Percent Gain' column\n",
    "QT['Percent Gain'] = (QT['2023-02-28'] / QT['2022-06-30'] - 1) * 100\n",
    "QT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bee8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QT = QT.drop(columns=['CountyName', '2022-06-30', '2023-02-28'], axis=1)\n",
    "QT.columns = ['zip', 'Percent Gain']\n",
    "QT = QT.set_index(['zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QT_IRS = pd.merge(QT, irs3, on='zip', how='inner')\n",
    "QT_IRS = QT_IRS.dropna()\n",
    "QT_IRS.isnull().sum()\n",
    "QT_IRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantile values\n",
    "qtq_low = QT_IRS.quantile(0.05)\n",
    "qtq_high = QT_IRS.quantile(0.95)\n",
    "# Filter out the outliers\n",
    "QT_IRS = QT_IRS[(QT_IRS >= qtq_low) & (QT_IRS <= qtq_high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as mcolors\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "colors = [(0, 'red'), (1, 'green')] \n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list('CustomColormap', colors)\n",
    "\n",
    "plt.scatter(QT_IRS['Avg_AGI'], QT_IRS['Percent Gain'], c=QT_IRS['Percent Gain'], cmap=custom_cmap)\n",
    "\n",
    "# Set the x-axis tick formatter to display the true units\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(False)\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "# Set other plot settings (labels, title, etc.)\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Home Value Percent Gain')\n",
    "plt.title('Income (2019) vs. Home Value Percentage Gain (2022-2023) by zip code')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ffc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "QT_sorted = QT.sort_values(by='Percent Gain', ascending=True)\n",
    "QT_sorted.head()\n",
    "\n",
    "QT.to_csv('tableau_QT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f531f53",
   "metadata": {},
   "source": [
    "## Total Assets vs SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file paths to the CSVs using the Path class from the pathlib library\n",
    "fed_assets_path = Path(\"../Resources/FRED corrected dates/total_assets.csv\")\n",
    "sp500_path = Path(\"../Resources/Yahoo all dates/sp500_data.csv\")\n",
    "# Read the ice cream sales data, set the `date` as the index\n",
    "fed_df = pd.read_csv(fed_assets_path)\n",
    "sp500_df = pd.read_csv(sp500_path, index_col=\"Date\", infer_datetime_format=True, parse_dates=True)\n",
    "\n",
    "sp500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Date', 'WALCL']\n",
    "fed_df.columns = columns\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3751568",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df['Date'] = pd.to_datetime(fed_df['Date'])\n",
    "fed_df.set_index('Date', inplace=True)\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df.index = pd.to_datetime(sp500_df.index)\n",
    "sp500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a63cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df['Open'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac542e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `concat` function to combine the two DataFrames by matching indexes (or in this case `Month`)\n",
    "combined_df = pd.concat([fed_df, sp500_df], axis=\"columns\", join=\"inner\")\n",
    "combined_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined_df.drop(columns=['High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "combined_c = ['Fed_Total_Assets', 'S&P']\n",
    "combined.columns = combined_c\n",
    "liqcombined = combined\n",
    "combined['% Gain Fed'] = combined['Fed_Total_Assets'].pct_change() * 100\n",
    "combined['% Gain S&P'] = combined['S&P'].pct_change() * 100\n",
    "combined.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a77314",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fed_Monthly_Return = combined['Fed_Total_Assets'].pct_change()\n",
    "SP500_Monthly_Return = combined['S&P'].pct_change()\n",
    "# Calculate the cumulative gain (or loss)\n",
    "combined['Fed_Cumulative_Gain'] = (1 + Fed_Monthly_Return).cumprod() - 1\n",
    "combined['S&P_Cumulative_Gain'] = (1 + SP500_Monthly_Return).cumprod() - 1\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5294644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization formula\n",
    "combined_norm = (combined - combined.min()) / (combined.max() - combined.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 9))\n",
    "# Line plot for Asset1_Price\n",
    "plt.plot(combined_norm.index, combined_norm['Fed_Total_Assets'], label='Fed_Total_Assets')\n",
    "\n",
    "# Line plot for Asset2_Price\n",
    "plt.plot(combined_norm.index, combined_norm['S&P'], label='S&P')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('S&P vs Fed Balance Sheet Normalized')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig('S&P vs Fed Balance Sheet Normalized.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c9577",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.plot(kind='scatter', x='Fed_Cumulative_Gain', y='S&P_Cumulative_Gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "# Line plot for Asset1_Price\n",
    "plt.plot(combined.index, combined['% Gain Fed'], label='% Gain Fed')\n",
    "\n",
    "# Line plot for Asset2_Price\n",
    "plt.plot(combined.index, combined['% Gain S&P'], label='% Gain S&P')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Monthly % Gain Comparison')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3872c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.plot(kind='scatter', x='Fed_Total_Assets', y='S&P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_correlation = combined.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidpath = Path(\"../Resources/FRED corrected dates/corporate_liquidity.csv\")\n",
    "liquid_df = pd.read_csv(liquidpath, parse_dates=True, index_col=\"date\", infer_datetime_format=True)\n",
    "liq_fed_sp = pd.concat([liquid_df, liqcombined], axis=1, join='inner')\n",
    "liq_fed_sp = liq_fed_sp.drop(columns={'% Gain Fed', '% Gain S&P'})\n",
    "price_correlation = liq_fed_sp.corr()\n",
    "sns.heatmap(price_correlation, cmap='coolwarm', annot=True, fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.savefig('S&P_Fed_Corporate.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b370f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27f3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24f943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504648ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvizenv] *",
   "language": "python",
   "name": "conda-env-pyvizenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
